{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Build SSD_VGG.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtKA25PKBjPP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "eaff297f-a24a-4744-994c-99a26c4a5ee7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r1h0QBSEO3fJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "9f050124-f999-4ab0-97a1-91bd8ceda69b"
      },
      "source": [
        "%cd /content/drive/My\\ Drive/cv/day29_31/Object_Detection"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cv/day29_31/Object_Detection\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQf_fcZVS-4x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        },
        "outputId": "44149be3-0036-420a-e2cd-2fe53d59d9f5"
      },
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \u001b[0m\u001b[01;34mannotations\u001b[0m/             \u001b[01;34mFaster_RCNN_Keras\u001b[0m/   ssd.py\n",
            "'Build RetinaNet.ipynb'   \u001b[01;34mimages\u001b[0m/              \u001b[01;34mutils\u001b[0m/\n",
            "'Build SSD_VGG.ipynb'     \u001b[01;34mlayers\u001b[0m/              輸入資料型態.ipynb\n",
            " \u001b[01;34mdata\u001b[0m/                    OHEM_Focal.png\n",
            " \u001b[01;34mdemo\u001b[0m/                    \u001b[01;34m__pycache__\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kUY3By1tBp53",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "2878d4d0-df34-4915-b5e4-39ad4c3b9da4"
      },
      "source": [
        "\"\"\"\n",
        "Ref\n",
        "1.\n",
        "https://blog.csdn.net/wfei101/article/details/78597442\n",
        "2.\n",
        "http://www.chenjianqu.com/show-92.html\n",
        "3.\n",
        "https://www.lizenghai.com/archives/35732.html\n",
        "4.\n",
        "https://hellozhaozheng.github.io/z_post/PyTorch-SSD/#MultiBox\n",
        "5.\n",
        "https://zhuanlan.zhihu.com/p/77736067\n",
        "6.\n",
        "https://www.twblogs.net/a/5c177b61bd9eee5e40bbc788\n",
        "7.\n",
        "https://zhuanlan.zhihu.com/p/85538108\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nRef\\n1.\\nhttps://blog.csdn.net/wfei101/article/details/78597442\\n2.\\nhttp://www.chenjianqu.com/show-92.html\\n3.\\nhttps://www.lizenghai.com/archives/35732.html\\n4.\\nhttps://hellozhaozheng.github.io/z_post/PyTorch-SSD/#MultiBox\\n5.\\nhttps://zhuanlan.zhihu.com/p/77736067\\n6.\\nhttps://www.twblogs.net/a/5c177b61bd9eee5e40bbc788\\n7.\\nhttps://zhuanlan.zhihu.com/p/85538108\\n\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "izJin0AoaLdV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from ssd import build_ssd\n",
        "\n",
        "## box_utils裡面很多Function，可以看看是怎麼設計的\n",
        "from layers.box_utils import * \n",
        "from torch.autograd import Variable\n",
        "from layers import functions\n",
        "from layers import modules\n",
        "from math import sqrt \n",
        "from itertools import product\n",
        "from torch.autograd import Function\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import argparse\n",
        "import pickle\n",
        "import torch.nn.functional as F\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJ6AahaQaLda",
        "colab_type": "code",
        "outputId": "182b97f9-be84-431f-bf2e-b795b91503e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "## 詳細模型結構可以參考ssd.py\n",
        "ssd_net=build_ssd('train', size=300, num_classes=21)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cv/day29_31/Object_Detection/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DlCZnvmPaLdd",
        "colab_type": "text"
      },
      "source": [
        "## 默認Config檔案在data/config.py內"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6M2fwc-WaLde",
        "colab_type": "code",
        "outputId": "01edf065-5c79-444b-b996-c37e56d1899d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "source": [
        "ssd_net.cfg"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
              " 'clip': True,\n",
              " 'feature_maps': [38, 19, 10, 5, 3, 1],\n",
              " 'lr_steps': (80000, 100000, 120000),\n",
              " 'max_iter': 120000,\n",
              " 'max_sizes': [60, 111, 162, 213, 264, 315],\n",
              " 'min_dim': 300,\n",
              " 'min_sizes': [30, 60, 111, 162, 213, 264],\n",
              " 'name': 'VOC',\n",
              " 'num_classes': 21,\n",
              " 'steps': [8, 16, 32, 64, 100, 300],\n",
              " 'variance': [0.1, 0.2]}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X8-vDy8QaLdh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cfg = {\n",
        "    'num_classes': 21,\n",
        "    'lr_steps': (80000, 100000, 120000),\n",
        "    'max_iter': 120000,\n",
        "    'feature_maps': [38, 19, 10, 5, 3, 1],\n",
        "    'min_dim': 300,\n",
        "    'steps': [8, 16, 32, 64, 100, 300],\n",
        "    'min_sizes': [30, 60, 111, 162, 213, 264],\n",
        "    'max_sizes': [60, 111, 162, 213, 264, 315],\n",
        "    'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
        "    'variance': [0.1, 0.2],\n",
        "    'clip': True,\n",
        "    'name': 'VOC',\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vf7YQG9zaLdk",
        "colab_type": "text"
      },
      "source": [
        "### 'aspect_ratios' : 使用六張Feature Map，每一張上方有預設的anchor boxes，Boxes aspect ratio可以自己設定\n",
        "### 'feature_maps' : 使用feature map大小為[38x38, 19x19, 10x10, 5x5, 3x3, 1x1]\n",
        "### 'min_sizes'、'max_sizes'可藉由下方算式算出，由作者自行設計\n",
        "### 'steps' : Feature map回放回原本300*300的比例，如38要回放為300大概就是8倍\n",
        "### 'variance' : Training 的一個trick，加速收斂，詳見：https://github.com/rykov8/ssd_keras/issues/53"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBGjeW2xaLdl",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st9W3ThPaLdl",
        "colab_type": "text"
      },
      "source": [
        "## 'min_sizes'、'max_sizes' 計算"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2NKH5UBYaLdm",
        "colab_type": "code",
        "outputId": "b788ec65-d867-4d61-e2db-4180cfb30c35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "import math\n",
        "## source:https://blog.csdn.net/gbyy42299/article/details/81235891\n",
        "min_dim = 300   ## 维度\n",
        "# conv4_3 ==> 38 x 38\n",
        "# fc7 ==> 19 x 19\n",
        "# conv6_2 ==> 10 x 10\n",
        "# conv7_2 ==> 5 x 5\n",
        "# conv8_2 ==> 3 x 3\n",
        "# conv9_2 ==> 1 x 1\n",
        "# https://www.twblogs.net/a/5c177b61bd9eee5e40bbc788\n",
        "mbox_source_layers = ['conv4_3', 'fc7', 'conv6_2', 'conv7_2', 'conv8_2', 'conv9_2'] ## prior_box來源層，可以更改。很多改進都是基於此處的調整。\n",
        "# in percent %\n",
        "min_ratio = 20 ## 這裡即是論文中所說的Smin的= 0.2，Smax的= 0.9的初始值，經過下面的運算即可得到min_sizes，max_sizes。\n",
        "max_ratio = 90\n",
        "\n",
        "# 以feature map上每個點的中點爲中心（offset=0.5），生成一些列同心的prior box(然後中心點的座標會乘以step，相當於從feature map位置映射回原圖位置)\n",
        "step = int(math.floor((max_ratio - min_ratio) / (len(mbox_source_layers) - 2)))## 取一個間距步長，即在下面用於循環給比取值時起一個間距作用。可以用一個具體的數值代替，這裡等於17。\n",
        "min_sizes = []  ## 經過以下運算得到min_sizes和max_sizes。\n",
        "max_sizes = []\n",
        "for ratio in range(min_ratio, max_ratio + 1, step): #20 37 54 71 88\n",
        "    ## 從min_ratio至max_ratio + 1每隔步驟(step)= 17取一個值，賦值給比(ratio)。注意範圍函數的作用。\n",
        "    ## min_sizes.append（）函數即把括號內部每次得到的值依次給了min_sizes。\n",
        "    # SSD 300中prior box設置並不能和paper中的公式對應\n",
        "    # min_size和max_size對應到的是最小和最大的正方形邊長\n",
        "    min_sizes.append(min_dim * ratio / 100.)  #[60.0,111.0,162.0,213.0,264.0]\n",
        "    max_sizes.append(min_dim * (ratio + step) / 100.)\n",
        "min_sizes = [min_dim * 10 / 100.] + min_sizes\n",
        "max_sizes = [min_dim * 20 / 100.] + max_sizes\n",
        "\n",
        "## steps: 這一步要仔細理解，即計算卷積層產生的prior_box距離原圖的步長(映射步長)，先驗框中心點的坐標會乘以step，\n",
        "## 相當於從特徵映射位置映射回原圖位置，比如conv4_3輸出特徵圖大小為38 *38，而輸入的圖片為300* 300，\n",
        "## 所以38 *8約等於300，所以映射步長為8.這是針對300* 300的訓練圖片。\n",
        "steps = [8, 16, 32, 64, 100, 300]  \n",
        "\n",
        "# min_size/max_size都是用於寬高比為1的box的預測.[2]用於預測宽高比為2:1和1:2的box.\n",
        "# [2,3]則預測4個shape, 1;2,2:1,1:3,3:1\n",
        "aspect_ratios = [[2], [2, 3], [2, 3], [2, 3], [2], [2]]\n",
        " \n",
        "print('min_sizes: ',min_sizes)\n",
        "print('max_sizes: ',max_sizes)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "min_sizes:  [30.0, 60.0, 111.0, 162.0, 213.0, 264.0]\n",
            "max_sizes:  [60.0, 111.0, 162.0, 213.0, 264.0, 315.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buFWEIj7aLdq",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g4oco1eRaLdr",
        "colab_type": "text"
      },
      "source": [
        "## Default anchor boxes設計原理，看懂收穫很多\n",
        "##### 可以理解 SSD原文中 8732個anchors是怎麼來的\n",
        "##### 38×38×4+19×19×6+10×10×6+5×5×6+3×3×4+1×1×4=8732"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V3LoeKi-aLdr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class PriorBox(object):\n",
        "    \"\"\"Compute priorbox coordinates in center-offset form for each source\n",
        "    feature map.\n",
        "    \"\"\"\n",
        "    # #cfg是定義在data/config.py裡面，是模型的相關配置\n",
        "    def __init__(self, cfg):\n",
        "        super(PriorBox, self).__init__()\n",
        "        self.image_size = cfg['min_dim']\n",
        "        # number of priors for feature map location (either 4 or 6)\n",
        "        # 輸出特徵圖的數量\n",
        "        self.num_priors = len(cfg['aspect_ratios'])\n",
        "        self.variance = cfg['variance'] or [0.1]\n",
        "        # 尺度\n",
        "        self.feature_maps = cfg['feature_maps']\n",
        "        # 計算先驗框用到的Smin和Smax\n",
        "        self.min_sizes = cfg['min_sizes']\n",
        "        self.max_sizes = cfg['max_sizes']\n",
        "        # 各個輸出特徵圖每個像素對應到原圖的大小\n",
        "        self.steps = cfg['steps']\n",
        "        self.aspect_ratios = cfg['aspect_ratios']\n",
        "        self.clip = cfg['clip']\n",
        "        self.version = cfg['name']\n",
        "        for v in self.variance:\n",
        "            if v <= 0:\n",
        "                raise ValueError('Variances must be greater than 0')\n",
        "\n",
        "    def forward(self):\n",
        "        mean = []\n",
        "        '''依照Feature map大小找出所有的pixel 中心'''\n",
        "        '''下方這兩個loop會找出W個x軸pixel對上W個y軸pixel，假如現在是在38x38的feature map上，就會有38x38個值'''\n",
        "        '''ex. [0,1],[0,2]..[0,37] [1,1],[1,2]..[1,37]..........[37,37]'''\n",
        "        for k, f in enumerate(self.feature_maps):\n",
        "            # product(A, repeat=4) 和 product(A, A, A, A) 是一樣的。\n",
        "            # product(A, B) 和 ((x,y) for x in A for y in B) 返回結果一樣。\n",
        "            # product(range(2), repeat=3) --> 000 001 010 011 100 101 110 111\n",
        "            # (0 , 0) ...(0 , 37) , ... (37 , 37)\n",
        "            for i, j in product(range(f), repeat=2):\n",
        "                f_k = self.image_size / self.steps[k] ## 如self.steps==8，就是先將原圖size normalize (/300)後再乘上8\n",
        "                # 是輸出特徵圖的下採樣率，則f_k是特徵圖的邊長\n",
        "                # unit center x,y\n",
        "                '''中心點'''\n",
        "                # 每個先驗框中心的計算公式是：(cx,cy)=((i+0.5)/|fk|, (j+0.5)/|fk|)，其中|fk|是特徵圖的邊長\n",
        "                # 計算中心點,這裡的j是沿x方向變化的\n",
        "                # (0.5 , 0.5) , (1.5 , 0.5) , (2.5 , 0.5)\n",
        "                cx = (j + 0.5) / f_k\n",
        "                cy = (i + 0.5) / f_k\n",
        "\n",
        "                # aspect_ratio: 1\n",
        "                # rel size: min_size\n",
        "                \"\"\"\n",
        "                每個特徵圖使用的先驗框大小：\n",
        "                Sk=Smin+(Smax-Smin)(k-1)/(m-1)，k的值[1,m]，m是輸出特徵圖的數量，\n",
        "                Sk是先驗框相對於整張圖片的比例，Smin=0.2，Smax=0.95\n",
        "                先驗框的寬度w_k_a=Sk*ar^0.5，高度h_k_a=Sk/ar^0.5，\n",
        "                其中ar是特徵圖的比例，有{3, 2, 1, 1/2, 1/3}\n",
        "                \"\"\"\n",
        "                '''/self.image_size 就是在做normalization '''\n",
        "                # aspect_ratio: 1\n",
        "                # rel size: min_size\n",
        "                s_k = self.min_sizes[k]/self.image_size\n",
        "                '''小的正方形box'''\n",
        "                mean += [cx, cy, s_k, s_k]\n",
        "\n",
        "                # aspect_ratio: 1\n",
        "                # rel size: sqrt(s_k * s_(k+1))\n",
        "                '''大的正方形box'''\n",
        "                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))\n",
        "                # 相當於extend \n",
        "                # extend: Extends list by appending elements from the iterable.\n",
        "                mean += [cx, cy, s_k_prime, s_k_prime]\n",
        "\n",
        "                # rest of aspect ratios\n",
        "                for ar in self.aspect_ratios[k]:\n",
        "                    '''aspect ratio 2,3'''\n",
        "                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]\n",
        "                    '''aspect ratio 1/2,1/3'''\n",
        "                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]\n",
        "        # back to torch land\n",
        "        # 此時的mean是1*34928的list，要4個數就分割出来，所以需要用view,從而變成[8732,4],即有8732个瞄點框\n",
        "\n",
        "        output = torch.Tensor(mean).view(-1, 4)\n",
        "        if self.clip:\n",
        "            # 對每個元素進行截斷限制,限制為[0,1]之間\n",
        "            output.clamp_(max=1, min=0)\n",
        "        return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VWkvbKJTaLdu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "PriorBox_Demo=PriorBox(cfg)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlODWKb0aLdx",
        "colab_type": "code",
        "outputId": "e451e5b7-7b98-4892-9255-e494ceea6abb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "print(PriorBox_Demo.forward().shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8732, 4])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2mBOFLFaLdz",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UvMYe4dZaLd0",
        "colab_type": "text"
      },
      "source": [
        "## Loss 如何設計"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qYpCyefWaLd0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiBoxLoss(nn.Module):\n",
        "\n",
        "    def __init__(self, num_classes, overlap_thresh, prior_for_matching,\n",
        "                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,\n",
        "                 use_gpu=True):\n",
        "        super(MultiBoxLoss, self).__init__()\n",
        "        self.use_gpu = use_gpu\n",
        "        '''有幾類'''\n",
        "        self.num_classes = num_classes\n",
        "        '''判定為正樣本的threshold，一般設為0.5'''\n",
        "        # 交集的閾值\n",
        "        self.threshold = overlap_thresh\n",
        "        '''background自己會有一類，不用Label，假如我們有20類一樣標註0-19，下方會自己空出一類給background'''\n",
        "        # 背景標籤, 0\n",
        "        self.background_label = bkg_label\n",
        "        # False\n",
        "        self.encode_target = encode_target\n",
        "        # True\n",
        "        self.use_prior_for_matching = prior_for_matching\n",
        "        '''OHEM，找出分得最不好的樣品，也就是confidence score比較低的正負樣品'''\n",
        "        # True\n",
        "        self.do_neg_mining = neg_mining\n",
        "        '''負樣品與正樣品的比例，通常是3:1'''\n",
        "        self.negpos_ratio = neg_pos\n",
        "        # 0.5 判定負樣本的閾值\n",
        "        self.neg_overlap = neg_overlap\n",
        "        self.variance = cfg['variance']\n",
        "     \n",
        "\n",
        "    def forward(self, predictions, targets):\n",
        "\n",
        "        '''prediction會output三個值'''\n",
        "        '''loc shape: bounding box 資訊，torch.size(batch_size,num_priors,4)'''\n",
        "        '''conf shape: 每一個bounding box 的信心程度，torch.size(batch_size,num_priors,num_classes)'''\n",
        "        '''priors shape: 預設的defaul box， torch.size(num_priors,4)'''\n",
        "        # 整個prediction是剛才forward的階段的輸出\n",
        "        loc_data, conf_data, priors = predictions\n",
        "        # num = batch_size\n",
        "        num = loc_data.size(0)\n",
        "        # [8732 , 4]\n",
        "        # loc_data.size(1) = 8732, 因此 priors 維持不變\n",
        "        priors = priors[:loc_data.size(1), :]\n",
        "        # num_priors = 8732\n",
        "        num_priors = (priors.size(0))\n",
        "        # num_classes = 21 (默認為voc數據集)\n",
        "        num_classes = self.num_classes\n",
        "\n",
        "        # match priors (default boxes) and ground truth boxes\n",
        "        loc_t = torch.Tensor(num, num_priors, 4)\n",
        "        conf_t = torch.LongTensor(num, num_priors)\n",
        "        for idx in range(num):\n",
        "            # targets是列表, 列表的長度為batch_size, 列表中每個元素為一個tensor,\n",
        "            # 其shape為[num_objs, 5], 其中num_objs為當前圖片中物體的數量, 第二維前4個元素為邊框坐標, 最後一個元素為類別編號(1~20)\n",
        "            # [num_objs, 4]\n",
        "            truths = targets[idx][:, :-1].data\n",
        "            labels = targets[idx][:, -1].data\n",
        "            # [8732, 4]\n",
        "            # .data 返回和 x 的相同數據tensor, 但不會加入到x的計算歷史裡\n",
        "            defaults = priors.data\n",
        "\n",
        "            # from ..box_utils import match\n",
        "            # 關鍵函數, 實現候選框與真實框之間的匹配, 注意是候選框而不是預測結果框! \n",
        "            '''jaccard 計算每一個BBOX與ground truth的IOU'''\n",
        "            # 注意! 要清楚 Python 中的參數傳遞機制, 此處在函數内部會改變 loc_t, conf_t 的值\n",
        "            # threshold:確定是否匹配的交集比閾值\n",
        "            # truths: (tensor: [num_obj, 4]) 存儲真實box的邊框坐標\n",
        "            # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存儲推薦框的坐標,注意,此時的框是default box,而不是SSD網路預測出来的框的坐標\n",
        "            # 預測的結果存儲在loc_data中,其shape為[num_obj, 8732, 4].\n",
        "            # 其中 num_objs 為當前圖片中物體的數量\n",
        "            # idx: batches 中圖片的序號, 標示當前正在處理的image在batches中的序號\n",
        "            # loc_t: (tensor: [batches, 8732, 4]),\n",
        "            # conf_t: (tensor: [batches, 8732])\n",
        "            match(self.threshold, truths, defaults, self.variance, labels,\n",
        "                  loc_t, conf_t, idx)\n",
        "        if self.use_gpu:\n",
        "            loc_t = loc_t.cuda()\n",
        "            conf_t = conf_t.cuda()\n",
        "        '''用Variable包裝'''\n",
        "        # 用Variable封裝loc_t,新版本的PyTorch無需這麼做,只需要將requires_grad屬性設置為True就行了\n",
        "        loc_t = Variable(loc_t, requires_grad=False)\n",
        "        conf_t = Variable(conf_t, requires_grad=False)\n",
        "\n",
        "        # 篩選出>0的box下標(大部分都是=0的)\n",
        "        pos = conf_t > 0\n",
        "        # 求和,取得滿足條件的box的數量,[batch_size, num_gt_threshold]\n",
        "        num_pos = pos.sum(dim=1, keepdim=True)\n",
        "\n",
        "        # torch.unsqueeze()這個函數主要是對數據維度進行擴充。給指定位置加上維數為一的維度\n",
        "        # b.expand_as(a)就是將b進行擴充，擴充到a的維度\n",
        "        # loc_data:[batch, num_priors, 4]\n",
        "        # pos: [batch, num_priors]\n",
        "        # pos_idx: [batch, num_priors, 4],複製下標成坐標格式, 以便獲取坐標值\n",
        "        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)\n",
        "        # 獲取預測結果值\n",
        "        # view相當於reshape\n",
        "        loc_p = loc_data[pos_idx].view(-1, 4)\n",
        "        # 獲取gt值\n",
        "        loc_t = loc_t[pos_idx].view(-1, 4)\n",
        "        '''smooth_l1_loss 計算bounding box regression'''\n",
        "        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False)\n",
        "\n",
        "        # Compute max conf across batch for hard negative mining\n",
        "        # 計算最大的置信度, 以進行難負樣本挖掘\n",
        "        # conf_data: [batch, num_priors, num_classes]\n",
        "        # batch_conf: [batch, num_priors, num_classes]\n",
        "        # reshape\n",
        "        batch_conf = conf_data.view(-1, self.num_classes)\n",
        "\n",
        "\n",
        "        # conf_t: [batch, num_priors]\n",
        "        # loss_c: [batch*num_priors, 1], 計算每個priorbox預測後的損失\n",
        "        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))\n",
        "\n",
        "        # Hard Negative Mining\n",
        "        # 難負樣本挖掘, 按照loss进行排序, 取loss最大的負樣本參與更新\n",
        "        loss_c = loss_c.view(num, -1)\n",
        "        loss_c[pos] = 0\n",
        "        '''排列confidence 的分數'''\n",
        "        _, loss_idx = loss_c.sort(1, descending=True)\n",
        "        _, idx_rank = loss_idx.sort(1)\n",
        "        num_pos = pos.long().sum(1, keepdim=True)\n",
        "        '''負樣品取出數量 == negpos_ratio*num_pos'''\n",
        "        num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)\n",
        "        # 獲取到負樣本的下標\n",
        "        neg = idx_rank < num_neg.expand_as(idx_rank)\n",
        "\n",
        "        # Confidence Loss Including Positive and Negative Examples\n",
        "        pos_idx = pos.unsqueeze(2).expand_as(conf_data)\n",
        "        neg_idx = neg.unsqueeze(2).expand_as(conf_data)\n",
        "        conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)\n",
        "        targets_weighted = conf_t[(pos+neg).gt(0)]\n",
        "        '''用cross_entropy做分類'''\n",
        "        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)\n",
        "\n",
        "        # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N\n",
        "        #double轉成torch.float64\n",
        "        N = num_pos.data.sum().double()\n",
        "        loss_l = loss_l.double()\n",
        "        loss_c = loss_c.double()\n",
        "        loss_l /= N\n",
        "        loss_c /= N\n",
        "        return loss_l, loss_c\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laApeAUuaLd3",
        "colab_type": "text"
      },
      "source": [
        "## 產生我們Loss function，注意這裡的class要包含背景"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQehKH-EaLd3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Use_cuda=False\n",
        "criterion = MultiBoxLoss(21, 0.5, True, 0, False, 3, 0.5,False, Use_cuda,)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_COK974CaLd5",
        "colab_type": "text"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nXrOMKR_aLd6",
        "colab_type": "text"
      },
      "source": [
        "## 基本設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8YDb-CUPaLd6",
        "colab_type": "code",
        "outputId": "48abeb9c-5e88-404f-fe4a-5f5963af7948",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "ssd_net=build_ssd('train', size=300, num_classes=21)\n",
        "use_pretrained=False\n",
        "if use_pretrained:\n",
        "    ssd_net.load_weights('./demo/ssd300_mAP_77.43_v2.pth')\n",
        "net=ssd_net"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/cv/day29_31/Object_Detection/ssd.py:34: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "  self.priors = Variable(self.priorbox.forward(), volatile=True)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TBIZE94kaLd9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''要不要使用gpu'''\n",
        "Use_cuda=False\n",
        "\n",
        "'''tensor type會依照cpu或gpu有所不同'''\n",
        "if torch.cuda.is_available():\n",
        "    if Use_cuda:\n",
        "        torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
        "    else:\n",
        "        print(\"WARNING: It looks like you have a CUDA device, but aren't \" +\n",
        "              \"using CUDA.\\nRun with --cuda for optimal training speed.\")\n",
        "        torch.set_default_tensor_type('torch.FloatTensor')\n",
        "else:\n",
        "    torch.set_default_tensor_type('torch.FloatTensor')\n",
        "\n",
        "\n",
        "'''使用GPU時模型要轉成cuda'''\n",
        "if Use_cuda:\n",
        "    net = net.cuda()\n",
        "    \n",
        "batch_size_=32\n",
        "optimizer = optim.Adam(net.parameters(),lr=0.00001/batch_size_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5twx9LIiaLd_",
        "colab_type": "text"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lre4SaVDaLd_",
        "colab_type": "text"
      },
      "source": [
        "## 訓練"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jmSvEA8aLeA",
        "colab_type": "text"
      },
      "source": [
        "## 這裡我們先示範輸入的 image,Label格式，真正在訓練時，準備成一樣格式即可"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzQ32DATaLeA",
        "colab_type": "code",
        "outputId": "825203b4-e47f-400c-cf4f-2286ebc836cb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "'''輸入影像格式，假設batch size 為 4'''\n",
        "image_in=torch.tensor(torch.rand(4,3,300,300),dtype=torch.float32)\n",
        "'''Label格式，沒有固定長度，看圖像中有幾個label就有幾個'''\n",
        "label_0=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
        "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
        "label_1=[[ 0.1804,  0.6076,  0.7701,  0.8485, 13.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 11.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 7.0000],\n",
        "       [ 0.2950,  0.0000,  0.8238,  0.3641, 5.0000],]\n",
        "label_2=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 14.0000],\n",
        "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]\n",
        "label_3=[[ 0.1804,  0.6076,  0.7701,  0.8485, 0.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 3.0000],\n",
        "       [ 0.2250,  0.0000,  0.9238,  0.5641, 19.0000],\n",
        "       [ 0.2950,  0.0000,  0.8238,  0.3641, 6.0000],]"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xO-OtFS7aLeD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs=1\n",
        "iteration=10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfw0ekaTaLeF",
        "colab_type": "code",
        "outputId": "98cd590f-92e5-4928-b793-5756f75a491c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "for epoch in range(epochs):\n",
        "    n=0\n",
        "    loss_sum=[]\n",
        "    loc_loss=[]\n",
        "    conf_loss=[]\n",
        "    for number__ in range(iteration) :\n",
        "        #########################################\n",
        "        print(number__)\n",
        "        #########################################\n",
        "        '''要用Variable包裝tensor才能送入模型'''\n",
        "        if Use_cuda:\n",
        "            image_ = Variable(image_in.cuda())\n",
        "            y = [Variable(torch.tensor(label_0).cuda(), volatile=True),Variable(torch.tensor(label_1).cuda(), \n",
        "                volatile=True),Variable(torch.tensor(label_2).cuda(), volatile=True),Variable(torch.tensor(label_3).cuda(), volatile=True)]      \n",
        "        else:\n",
        "            image_ = Variable(image_in)\n",
        "            y = [Variable(torch.tensor(label_0), volatile=True),Variable(torch.tensor(label_1), \n",
        "                volatile=True),Variable(torch.tensor(label_2), volatile=True),Variable(torch.tensor(label_3), volatile=True)]\n",
        "\n",
        "        '''Forward Pass'''\n",
        "        out = net(image_)\n",
        "        '''Regression Loss and Classification Loss'''\n",
        "        loss_l,loss_c = criterion(out,y )\n",
        "        loss = loss_l+ loss_c\n",
        "        '''Backward'''\n",
        "        loss.backward()\n",
        "\n",
        "        loc_loss.append(loss_l.data.cpu().numpy())\n",
        "        conf_loss.append(loss_c.data.cpu().numpy())\n",
        "        loss_sum.append(loss.data.cpu().numpy())\n",
        "        '''更新參數'''\n",
        "        optimizer.step()\n",
        "        '''清空Gradients'''\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        n+=1\n",
        "        if n%10==0:\n",
        "            print('BBOX Regression Loss: ', np.mean(loc_loss))\n",
        "            print('Classification Loss: ', np.mean(conf_loss))\n",
        "    '''儲存權重'''\n",
        "   #torch.save(ssd_net.state_dict(),'./drive/My Drive/cv/day29_31/Object_Detection/weights.pth')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:17: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:18: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
            "/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
            "  warnings.warn(warning.format(ret))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "BBOX Regression Loss:  2.5144689489293985\n",
            "Classification Loss:  12.243745365849247\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjkGrgrFaLeI",
        "colab_type": "text"
      },
      "source": [
        "## 想要Train VOC,COCO可以參考原Github:https://github.com/amdegroot/ssd.pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MkainnpaLeJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}