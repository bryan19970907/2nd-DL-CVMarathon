{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7"
    },
    "colab": {
      "name": "Day015_Cifar_HW.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FAHKbczYTbk",
        "colab_type": "text"
      },
      "source": [
        "## 『本次練習內容』\n",
        "#### 運用這幾天所學觀念搭建一個CNN分類器"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EndF9ChQYTbp",
        "colab_type": "text"
      },
      "source": [
        "## 『本次練習目的』\n",
        "  #### 熟悉CNN分類器搭建步驟與原理\n",
        "  #### 學員們可以嘗試不同搭法，如使用不同的Maxpooling層，用GlobalAveragePooling取代Flatten等等"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EKYZ7CtmYTbs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 79
        },
        "outputId": "538433fb-e104-48bb-83ee-dcd1c9e3e440"
      },
      "source": [
        "from keras.models import Sequential\n",
        "from keras.layers import Convolution2D\n",
        "from keras.layers import MaxPooling2D\n",
        "from keras.layers import Flatten\n",
        "from keras.layers import Dense\n",
        "from keras.layers import Dropout\n",
        "from keras.layers import BatchNormalization\n",
        "from keras.datasets import cifar10\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMtyOdt_YTb1",
        "colab_type": "code",
        "outputId": "c6de0e17-e0d1-46ea-fc42-6aa688bf3600",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print(x_train.shape) #(50000, 32, 32, 3)\n",
        "\n",
        "## Normalize Data\n",
        "def normalize(X_train,X_test):\n",
        "        mean = np.mean(X_train,axis=(0,1,2,3))\n",
        "        std = np.std(X_train, axis=(0, 1, 2, 3))\n",
        "        X_train = (X_train-mean)/(std+1e-7)\n",
        "        X_test = (X_test-mean)/(std+1e-7) \n",
        "        return X_train, X_test,mean,std\n",
        "    \n",
        "    \n",
        "## Normalize Training and Testset    \n",
        "x_train, x_test,mean_train,std_train = normalize(x_train, x_test) "
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 14s 0us/step\n",
            "(50000, 32, 32, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIbuAWifYTb6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "078112e3-dd20-48b4-eb89-966a7c528255"
      },
      "source": [
        "## OneHot Label 由(None, 1)-(None, 10)\n",
        "## ex. label=2,變成[0,0,1,0,0,0,0,0,0,0]\n",
        "print(y_train.shape)\n",
        "one_hot=OneHotEncoder()\n",
        "# https://stackoverflow.com/questions/52846604/using-toarray-with-onehotencoding-during-data-preprocessing\n",
        "y_train=one_hot.fit_transform(y_train).toarray()\n",
        "y_test=one_hot.transform(y_test).toarray()\n",
        "print(y_train.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(50000, 1)\n",
            "(50000, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C6einewYYTb-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        },
        "outputId": "13d817e7-12d3-41a0-d2ac-9f04c0dab799"
      },
      "source": [
        "\n",
        "classifier=Sequential()\n",
        "\n",
        "#卷積組合\n",
        "#32,3,3,input_shape=(32,32,3),activation='relu''\n",
        "classifier.add(Convolution2D(filters = 32 , kernel_size=(3 , 3) , strides= (1 , 1) , padding='same' , activation='relu' , input_shape = (32 , 32 , 3)))\n",
        "classifier.add(BatchNormalization(momentum=0.99 , epsilon=0.001))\n",
        "\n",
        "'''自己決定MaxPooling2D放在哪裡'''\n",
        "classifier.add(MaxPooling2D(pool_size=(2,2)))\n",
        "\n",
        "#卷積組合\n",
        "classifier.add(Convolution2D(filters = 32 , kernel_size=(3 , 3) , strides= (1 , 1) , padding='same' , activation='relu'))\n",
        "classifier.add(BatchNormalization(momentum=0.99 , epsilon=0.001))\n",
        "\n",
        "#flatten\n",
        "classifier.add(Flatten())\n",
        "\n",
        "#FC\n",
        "classifier.add(Dense(units=100 , activation='relu')) #output_dim=100,activation=relu\n",
        "\n",
        "#輸出\n",
        "classifier.add(Dense(output_dim=10,activation='softmax'))\n",
        "\n",
        "#超過兩個就要選categorical_crossentrophy\n",
        "classifier.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
        "classifier.fit(x_train,y_train,batch_size=128,epochs=20)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:23: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(activation=\"softmax\", units=10)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "50000/50000 [==============================] - 6s 112us/step - loss: 1.2539 - acc: 0.5656\n",
            "Epoch 2/20\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 0.7859 - acc: 0.7250\n",
            "Epoch 3/20\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 0.5457 - acc: 0.8099\n",
            "Epoch 4/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.3586 - acc: 0.8757\n",
            "Epoch 5/20\n",
            "50000/50000 [==============================] - 5s 101us/step - loss: 0.2237 - acc: 0.9236\n",
            "Epoch 6/20\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 0.1493 - acc: 0.9511\n",
            "Epoch 7/20\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 0.1128 - acc: 0.9614\n",
            "Epoch 8/20\n",
            "50000/50000 [==============================] - 5s 100us/step - loss: 0.1034 - acc: 0.9641\n",
            "Epoch 9/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.1033 - acc: 0.9653\n",
            "Epoch 10/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0876 - acc: 0.9708\n",
            "Epoch 11/20\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 0.0733 - acc: 0.9753\n",
            "Epoch 12/20\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 0.0665 - acc: 0.9777\n",
            "Epoch 13/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0607 - acc: 0.9790\n",
            "Epoch 14/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0622 - acc: 0.9789\n",
            "Epoch 15/20\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 0.0580 - acc: 0.9809\n",
            "Epoch 16/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0486 - acc: 0.9834\n",
            "Epoch 17/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0574 - acc: 0.9801\n",
            "Epoch 18/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0445 - acc: 0.9846\n",
            "Epoch 19/20\n",
            "50000/50000 [==============================] - 5s 99us/step - loss: 0.0404 - acc: 0.9863\n",
            "Epoch 20/20\n",
            "50000/50000 [==============================] - 5s 98us/step - loss: 0.0482 - acc: 0.9838\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1031d0a7f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JooEvMaYTcC",
        "colab_type": "text"
      },
      "source": [
        "## 預測新圖片，輸入影像前處理要與訓練時相同\n",
        "#### ((X-mean)/(std+1e-7) ):這裡的mean跟std是訓練集的\n",
        "## 維度如下方示範"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AmC_ngsZYTcD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "be2a9052-4e06-4894-ae90-c26d019f3cdd"
      },
      "source": [
        "input_example=(np.zeros(shape=(1,32,32,3))-mean_train)/(std_train+1e-7) \n",
        "classifier.predict_classes(input_example)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([4])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LJCZUIgniSc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "29c91b92-f1c7-4455-f0d3-d14f24841e83"
      },
      "source": [
        "result = classifier.evaluate(x_test , y_test)\n",
        "print(result[1])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 1s 106us/step\n",
            "0.6586\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}